# ============================================================
# æ–‡ä»¶è¯´æ˜: polling_data_processor.py - æ•°æ®å¤„ç†å’Œç¼“å­˜ç®¡ç†
# ============================================================
# åŠŸèƒ½:
#   1. è§£æå™¨å’Œè½¬æ¢å™¨åˆå§‹åŒ–
#   2. å†…å­˜ç¼“å­˜ç®¡ç† (æœ€æ–°æ•°æ®ä¾›APIè¯»å–)
#   3. æ‰¹é‡å†™å…¥ç¼“å­˜ (åŒé€Ÿè½®è¯¢æ¶æ„)
#   4. æ•°æ®å¤„ç†å‡½æ•° (_process_*)
#   5. è¶é˜€çŠ¶æ€é˜Ÿåˆ—ç®¡ç†
#   6. æ‰¹é‡å†™å…¥ InfluxDB
# ============================================================
# ã€æ•°æ®åº“å†™å…¥è¯´æ˜ã€‘
# ============================================================
# 1: DB32 ä¼ æ„Ÿå™¨æ•°æ® (å†™å…¥ InfluxDB)
#    - è½®è¯¢é—´éš”: 0.5ç§’
#    - æ‰¹é‡å†™å…¥: 30æ¬¡è½®è¯¢åå†™å…¥ (15ç§’)
#    - å†™å…¥æ¡ä»¶: å¿…é¡»æœ‰æ‰¹æ¬¡å·(batch_code)ä¸”å†¶ç‚¼çŠ¶æ€ä¸ºrunning/paused
#    - æ•°æ®ç‚¹:
#      * ç”µææ·±åº¦: LENTH1/2/3 (distance_mm, high_word, low_word)
#      * å†·å´æ°´å‹åŠ›: WATER_PRESS_1/2 (value kPa, raw)
#      * å†·å´æ°´æµé‡: WATER_FLOW_1/2 (value mÂ³/h, raw)
#      * å†·å´æ°´ç´¯è®¡: furnace_shell_water_total, furnace_cover_water_total (æ¯15ç§’è®¡ç®—)
#      * ç‚‰çš®-ç‚‰ç›–å‹å·®: |ç‚‰çš®å‹åŠ› - ç‚‰ç›–å‹åŠ›| (kPa)
# ============================================================
# 2: DB1 å¼§æµå¼§å‹æ•°æ® (å†™å…¥ InfluxDB)
#    - è½®è¯¢é—´éš”: 5ç§’(é»˜è®¤) / 0.2ç§’(å†¶ç‚¼ä¸­)
#    - æ‰¹é‡å†™å…¥: 20æ¬¡è½®è¯¢åå†™å…¥ (4ç§’)
#    - å†™å…¥æ¡ä»¶: å¿…é¡»æœ‰æ‰¹æ¬¡å·(batch_code)ä¸”å†¶ç‚¼çŠ¶æ€ä¸ºrunning/paused
#    - æ•°æ®ç‚¹:
#      * å¼§æµ: arc_current_U/V/W (A) - æ¯æ¬¡å†™å…¥
#      * å¼§å‹: arc_voltage_U/V/W (V) - æ¯æ¬¡å†™å…¥
#      * åŠŸç‡: power_U/V/W (kW), power_total (kW) - æ¯æ¬¡å†™å…¥
#      * è®¾å®šå€¼: arc_current_setpoint_U/V/W (A) - ä»…å˜åŒ–æ—¶å†™å…¥
#      * æ­»åŒº: manual_deadzone_percent (%) - ä»…å˜åŒ–æ—¶å†™å…¥
# ============================================================
# 2.1: èƒ½è€—æ•°æ® (å®šæ—¶è®¡ç®—å†™å…¥)
#    - è®¡ç®—é—´éš”: æ¯15ç§’è®¡ç®—ä¸€æ¬¡
#    - å†™å…¥æ–¹å¼: è®¡ç®—å®Œæˆåç«‹å³å†™å…¥
#    - æ•°æ®ç‚¹:
#      * ç´¯è®¡èƒ½è€—: energy_U/V/W_total (kWh), energy_total (kWh)
# ============================================================
# 3: æ–™ä»“é‡é‡æ•°æ® (å†™å…¥ InfluxDB)
#    - è½®è¯¢é—´éš”: 0.5ç§’ (ä¸DB32åŒæ­¥)
#    - æ‰¹é‡å†™å…¥: 30æ¬¡è½®è¯¢åå†™å…¥ (15ç§’)
#    - å†™å…¥æ¡ä»¶: å¿…é¡»æœ‰æ‰¹æ¬¡å·(batch_code)ä¸”å†¶ç‚¼çŠ¶æ€ä¸ºrunning/paused
#    - æ•°æ®ç‚¹:
#      * å‡€é‡: net_weight (kg)
#      * æŠ•æ–™ç´¯è®¡: feeding_total (kg) - æ¯30ç§’è®¡ç®—ä¸€æ¬¡å¢é‡
#    - ä¸å†™å…¥æ•°æ®åº“:
#      * æŠ•æ–™çŠ¶æ€: is_discharging (ä»…å†…å­˜ç¼“å­˜ï¼Œç”¨äºè®¡ç®—æŠ•æ–™ç´¯è®¡)
# ============================================================
# 4: ä¸å†™å…¥æ•°æ®åº“çš„æ•°æ® (ä»…å†…å­˜ç¼“å­˜)
#    - DB30 é€šä¿¡çŠ¶æ€ (ModbusStatusParser)
#    - DB41 æ•°æ®çŠ¶æ€ (DataStateParser)
#    - è¶é˜€çŠ¶æ€é˜Ÿåˆ— (ValveStatusMonitor) - ä»…ç”¨äºå†å²è®°å½•API
# ============================================================

import threading
import traceback
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List
from collections import deque

from backend.core.influxdb import write_points_batch, build_point
from backend.plc.parser_config_db32 import ConfigDrivenDB32Parser
from backend.plc.parser_config_db1 import ConfigDrivenDB1Parser
from backend.plc.parser_status import ModbusStatusParser
from backend.plc.parser_status_db41 import DataStateParser
from backend.tools.converter_furnace import FurnaceConverter
from backend.tools.converter_elec_db1_simple import (
    convert_db1_arc_data_simple,
    convert_to_influx_fields_simple,
    convert_to_influx_fields_with_change_detection,
    ArcDataSimple,
)
from backend.services.feeding_accumulator import get_feeding_accumulator
from backend.services.power_energy_calculator import get_power_energy_calculator


# ============================================================
# è§£æå™¨ä¸è½¬æ¢å™¨å®ä¾‹
# ============================================================
_modbus_parser: Optional[ConfigDrivenDB32Parser] = None  # DB32 ä¼ æ„Ÿå™¨è§£æå™¨
_db1_parser: Optional[ConfigDrivenDB1Parser] = None      # DB1 å¼§æµå¼§å‹è§£æå™¨
_status_parser: Optional[ModbusStatusParser] = None       # DB30 çŠ¶æ€è§£æå™¨
_db41_parser: Optional[DataStateParser] = None            # DB41 æ•°æ®çŠ¶æ€è§£æå™¨
_db18_parser: Optional['HopperDB18Parser'] = None         # DB18 æ–™ä»“ç”µæ°”æ•°æ®è§£æå™¨
_furnace_converter: Optional[FurnaceConverter] = None     # æ•°æ®è½¬æ¢å™¨

# ============================================================
# å†…å­˜ç¼“å­˜ (ä¾› API ç›´æ¥è¯»å–)
# ============================================================
_data_lock = threading.Lock()

# æœ€æ–°ä¼ æ„Ÿå™¨æ•°æ®ç¼“å­˜ (DB32)
_latest_modbus_data: Dict[str, Any] = {}
_latest_modbus_timestamp: Optional[datetime] = None

# æœ€æ–°å¼§æµå¼§å‹ç¼“å­˜ (DB1)
_latest_arc_data: Dict[str, Any] = {}
_latest_arc_timestamp: Optional[datetime] = None

# æœ€æ–°é€šä¿¡çŠ¶æ€ç¼“å­˜ (DB30)
_latest_status_data: Dict[str, Any] = {}
_latest_status_timestamp: Optional[datetime] = None

# æœ€æ–°æ•°æ®çŠ¶æ€ç¼“å­˜ (DB41)
_latest_db41_data: Dict[str, Any] = {}
_latest_db41_timestamp: Optional[datetime] = None

# æœ€æ–°æ–™ä»“é‡é‡ç¼“å­˜ (Modbus RTU)
_latest_weight_data: Dict[str, Any] = {}
_latest_weight_timestamp: Optional[datetime] = None

# æœ€æ–°æ–™ä»“ä¸Šé™å€¼ç¼“å­˜ (DB18)
_latest_hopper_upper_limit: float = 4900.0  # é»˜è®¤4900kg
_latest_hopper_upper_limit_timestamp: Optional[datetime] = None

# ============================================================
# è®¾å®šå€¼å˜åŒ–æ£€æµ‹ç¼“å­˜ (ç”¨äºæ™ºèƒ½å†™å…¥æ•°æ®åº“)
# ============================================================
# ä¸Šä¸€æ¬¡çš„è®¾å®šå€¼ (U, V, W)
_prev_setpoints: Optional[tuple] = None
# ä¸Šä¸€æ¬¡çš„æ‰‹åŠ¨æ­»åŒºç™¾åˆ†æ¯”
_prev_deadzone: Optional[float] = None

# ============================================================
# ç´¯è®¡å€¼å˜åŒ–æ£€æµ‹ç¼“å­˜ (ç”¨äºæ™ºèƒ½å†™å…¥æ•°æ®åº“)
# ============================================================
# ä¸Šä¸€æ¬¡å†™å…¥æ•°æ®åº“çš„æŠ•æ–™ç´¯è®¡å€¼
_prev_feeding_total: float = 0.0
# ä¸Šä¸€æ¬¡å†™å…¥æ•°æ®åº“çš„å†·å´æ°´ç´¯è®¡å€¼
_prev_furnace_shell_water_total: float = 0.0
_prev_furnace_cover_water_total: float = 0.0
# ä¸Šä¸€æ¬¡å†™å…¥æ•°æ®åº“çš„èƒ½è€—ç´¯è®¡å€¼
_prev_energy_total: float = 0.0

# ============================================================
# è¶é˜€çŠ¶æ€é˜Ÿåˆ—ç¼“å­˜ (Valve Status Queue Cache)
# ============================================================
# æ¯ä¸ªè¶é˜€ç»´æŠ¤ä¸€ä¸ªé˜Ÿåˆ—ï¼Œå­˜å‚¨æœ€è¿‘100æ¬¡çš„å¼€å…³çŠ¶æ€
# çŠ¶æ€æ ¼å¼: "10" (å…³é—­), "01" (æ‰“å¼€), "11" (å¼‚å¸¸), "00" (æœªçŸ¥)
_valve_status_queues: Dict[int, deque] = {
    1: deque(maxlen=100),  # è¶é˜€1çŠ¶æ€é˜Ÿåˆ—
    2: deque(maxlen=100),  # è¶é˜€2çŠ¶æ€é˜Ÿåˆ—
    3: deque(maxlen=100),  # è¶é˜€3çŠ¶æ€é˜Ÿåˆ—
    4: deque(maxlen=100),  # è¶é˜€4çŠ¶æ€é˜Ÿåˆ—
}
_valve_status_timestamps: Dict[int, deque] = {
    1: deque(maxlen=100),
    2: deque(maxlen=100),
    3: deque(maxlen=100),
    4: deque(maxlen=100),
}

# ============================================================
# æ‰¹é‡å†™å…¥ç¼“å­˜ (åŒé€Ÿè½®è¯¢æ¶æ„)
# ============================================================
#  å¼§æµå¼§å‹ç¼“å­˜ (é«˜é¢‘å†™å…¥)
# - è½®è¯¢é—´éš”: 0.2s
# - æ‰¹é‡å¤§å°: 20æ¬¡ (0.2sÃ—20=4så†™å…¥ä¸€æ¬¡)
_arc_buffer: deque = deque(maxlen=500)
_arc_buffer_count = 0
_arc_batch_size = 20  # 20æ¬¡å¼§æµè½®è¯¢åæ‰¹é‡å†™å…¥ (0.2sÃ—20=4s)

# ğŸ“Š æ™®é€šæ•°æ®ç¼“å­˜ (å¸¸è§„å†™å…¥)
# - è½®è¯¢é—´éš”: 5s
# - æ‰¹é‡å¤§å°: 20æ¬¡ (5sÃ—20=100så†™å…¥ä¸€æ¬¡)
_normal_buffer: deque = deque(maxlen=1000)
_normal_buffer_count = 0
_normal_batch_size = 20  # 20æ¬¡å¸¸è§„è½®è¯¢åæ‰¹é‡å†™å…¥ (5sÃ—20=100s)

# ============================================================
# ç»Ÿè®¡ä¿¡æ¯
# ============================================================
_stats = {
    "total_polls": 0,
    "successful_writes": 0,
    "failed_writes": 0,
    "last_poll_time": None,
    "db32_errors": 0,
    "db1_errors": 0,
    "modbus_errors": 0,
}


# ============================================================
# 1: è§£æå™¨åˆå§‹åŒ–æ¨¡å—
# ============================================================
def init_parsers():
    """åˆå§‹åŒ–è§£æå™¨"""
    global _modbus_parser, _db1_parser, _status_parser, _db41_parser, _db18_parser, _furnace_converter
    
    if _modbus_parser is None:
        try:
            _modbus_parser = ConfigDrivenDB32Parser()
            print(" DB32 ä¼ æ„Ÿå™¨æ•°æ®è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f" DB32 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    if _db1_parser is None:
        try:
            _db1_parser = ConfigDrivenDB1Parser()
            print(" DB1 å¼§æµå¼§å‹è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f" DB1 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    if _status_parser is None:
        try:
            _status_parser = ModbusStatusParser()
            print(" DB30 çŠ¶æ€è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f" DB30 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    if _db18_parser is None:
        try:
            from backend.plc.parser_hopper_db18 import HopperDB18Parser
            _db18_parser = HopperDB18Parser()
            print(" DB18 æ–™ä»“ç”µæ°”æ•°æ®è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f" DB18 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    if _db41_parser is None:
        try:
            _db41_parser = DataStateParser()
            print(" DB41 æ•°æ®çŠ¶æ€è§£æå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f" DB41 è§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
    if _furnace_converter is None:
        _furnace_converter = FurnaceConverter()
        print(" ç”µç‚‰æ•°æ®è½¬æ¢å™¨å·²åˆå§‹åŒ–")


def get_parsers():
    """è·å–è§£æå™¨å®ä¾‹ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰
    
    Returns:
        tuple: (db1_parser, modbus_parser, status_parser, db41_parser, db18_parser)
        
    æ³¨æ„: polling_loops_v2.py ä½¿ç”¨å…ƒç»„æ ¼å¼è°ƒç”¨æ­¤å‡½æ•°
    """
    return _db1_parser, _modbus_parser, _status_parser, _db41_parser, _db18_parser


def get_parsers_dict():
    """è·å–è§£æå™¨å®ä¾‹ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    
    Returns:
        dict: åŒ…å«æ‰€æœ‰è§£æå™¨å’Œè½¬æ¢å™¨çš„å­—å…¸
    """
    return {
        'db32_parser': _modbus_parser,
        'db1_parser': _db1_parser,
        'db30_parser': _status_parser,
        'db41_parser': _db41_parser,
        'db18_parser': _db18_parser,
        'converter': _furnace_converter
    }


# ============================================================
# 2: æ•°æ®å¤„ç†å‡½æ•°æ¨¡å—
# ============================================================
def process_modbus_data(raw_data: bytes):
    """å¤„ç† DB32 ä¼ æ„Ÿå™¨æ•°æ®
    
    æ•°æ®åŒ…å«: çº¢å¤–æµ‹è·, å‹åŠ›, æµé‡, è¶é˜€çŠ¶æ€
    æ–°å¢: å†·å´æ°´æµé‡è®¡ç®— (0.5sè½®è¯¢, 15ç§’ç´¯è®¡)
    ä¼˜åŒ–: å†·å´æ°´ç´¯è®¡å€¼åªåœ¨å˜åŒ–æ—¶å†™å…¥æ•°æ®åº“
    """
    global _latest_modbus_data, _latest_modbus_timestamp
    global _prev_furnace_shell_water_total, _prev_furnace_cover_water_total
    
    if not _modbus_parser:
        return

    try:
        # 1. è§£æåŸå§‹æ•°æ®
        parsed = _modbus_parser.parse_all(raw_data)
        
        # ========================================
        # 2. å†·å´æ°´æµé‡è®¡ç®— (æ–°å¢é€»è¾‘)
        # ========================================
        from backend.services.cooling_water_calculator import get_cooling_water_calculator
        from backend.services.batch_service import get_batch_service
        
        cooling_calc = get_cooling_water_calculator()
        batch_service = get_batch_service()
        
        # æå–å†·å´æ°´æ•°æ®
        # æ˜ å°„å…³ç³»:
        # - WATER_FLOW_1 (offset 16) -> ç‚‰çš®æµé‡
        # - WATER_FLOW_2 (offset 18) -> ç‚‰ç›–æµé‡
        # - WATER_PRESS_1 (offset 12) -> ç‚‰çš®æ°´å‹ (è¿‡æ»¤å™¨è¿›å£)
        # - WATER_PRESS_2 (offset 14) -> ç‚‰ç›–æ°´å‹ (è¿‡æ»¤å™¨å‡ºå£)
        cooling_flows = parsed.get('cooling_flows', {})
        cooling_pressures = parsed.get('cooling_pressures', {})
        
        # æµé‡æå– (mÂ³/h)
        flow_1_data = cooling_flows.get('WATER_FLOW_1', {})
        flow_2_data = cooling_flows.get('WATER_FLOW_2', {})
        furnace_shell_flow = flow_1_data.get('flow', 0.0) if isinstance(flow_1_data, dict) else 0.0
        furnace_cover_flow = flow_2_data.get('flow', 0.0) if isinstance(flow_2_data, dict) else 0.0
        
        # å‹åŠ›æå– (åŸå§‹å•ä½ Ã—0.01 kPa)
        press_1_data = cooling_pressures.get('WATER_PRESS_1', {})
        press_2_data = cooling_pressures.get('WATER_PRESS_2', {})
        furnace_shell_pressure = press_1_data.get('pressure', 0.0) if isinstance(press_1_data, dict) else 0.0
        furnace_cover_pressure = press_2_data.get('pressure', 0.0) if isinstance(press_2_data, dict) else 0.0
        
        # ã€ä¿®å¤ã€‘åªæœ‰åœ¨è¿è¡ŒçŠ¶æ€æ—¶æ‰æ·»åŠ æµ‹é‡æ•°æ®å’Œè®¡ç®—ç´¯è®¡
        if batch_service.is_running:
            # æ·»åŠ æµ‹é‡æ•°æ®å¹¶è·å–å‹å·®
            cooling_result = cooling_calc.add_measurement(
                furnace_cover_flow=furnace_cover_flow,
                furnace_shell_flow=furnace_shell_flow,
                furnace_cover_pressure=furnace_cover_pressure,
                furnace_shell_pressure=furnace_shell_pressure,
            )
            
            # è®¡ç®—åçš„å‹å·®å­˜å…¥ parsed ä¾›åç»­ä½¿ç”¨
            parsed['filter_pressure_diff'] = {
                'value': cooling_result['pressure_diff'],
                'unit': 'kPa'
            }
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—ç´¯è®¡æµé‡ (æ¯15ç§’)
            if cooling_result['should_calc_volume']:
                volume_result = cooling_calc.calculate_volume_increment()
                # æ›´æ–°ç´¯è®¡æµé‡åˆ° parsed
                parsed['furnace_cover_total_volume'] = volume_result['furnace_cover_total']
                parsed['furnace_shell_total_volume'] = volume_result['furnace_shell_total']
            else:
                # ä½¿ç”¨ç¼“å­˜çš„ç´¯è®¡å€¼
                volumes = cooling_calc.get_total_volumes()
                parsed['furnace_cover_total_volume'] = volumes['furnace_cover']
                parsed['furnace_shell_total_volume'] = volumes['furnace_shell']
        else:
            # ã€ä¿®å¤ã€‘ç»ˆæ­¢å†¶ç‚¼åï¼Œåªè®¡ç®—å‹å·®ï¼Œä¸ç´¯è®¡æµé‡
            parsed['filter_pressure_diff'] = {
                'value': furnace_shell_pressure - furnace_cover_pressure,
                'unit': 'kPa'
            }
            # ä½¿ç”¨ç¼“å­˜çš„ç´¯è®¡å€¼ï¼ˆä¸å†å¢åŠ ï¼‰
            volumes = cooling_calc.get_total_volumes()
            parsed['furnace_cover_total_volume'] = volumes['furnace_cover']
            parsed['furnace_shell_total_volume'] = volumes['furnace_shell']
        
        # 3. æ›´æ–°å†…å­˜ç¼“å­˜ (ä¾›å®æ—¶APIä½¿ç”¨)
        with _data_lock:
            _latest_modbus_data = parsed
            _latest_modbus_timestamp = datetime.now()
            
            # ========================================
            # è¶é˜€çŠ¶æ€é˜Ÿåˆ—æ›´æ–°é€»è¾‘ (æ—§ç‰ˆ - ä»…ç”¨äºå†å²è®°å½•API)
            # ========================================
            valve_status_data = parsed.get('valve_status', {})
            valve_status_byte = valve_status_data.get('raw_byte', 0)
            timestamp = datetime.now(timezone.utc)
            
            # è§£ææ¯ä¸ªè¶é˜€çš„2-bitçŠ¶æ€
            for valve_id in range(1, 5):  # è¶é˜€1-4
                bit_offset = (valve_id - 1) * 2
                bit_close = (valve_status_byte >> bit_offset) & 0x01
                bit_open = (valve_status_byte >> (bit_offset + 1)) & 0x01
                
                # ç»„åˆæˆçŠ¶æ€å­—ç¬¦ä¸²: "10"(å…³), "01"(å¼€), "11"(å¼‚å¸¸), "00"(æœªçŸ¥)
                status = f"{bit_close}{bit_open}"
                
                # æ·»åŠ åˆ°é˜Ÿåˆ—
                _valve_status_queues[valve_id].append(status)
                _valve_status_timestamps[valve_id].append(timestamp.isoformat())
        
        # ========================================
        # 3.1 å†™å…¥ç»Ÿä¸€ç¼“å­˜ DataCache + å‘é€ä¿¡å· DataBridge
        # ========================================
        try:
            from backend.bridge.data_cache import get_data_cache
            from backend.bridge.data_bridge import get_data_bridge
            import time
            
            data_cache = get_data_cache()
            data_bridge = get_data_bridge()
            
            # è·å–è¶é˜€å¼€åº¦æ•°æ®
            from backend.services.valve_calculator_service import get_all_valve_openness
            valve_openness_list = get_all_valve_openness()
            valve_openness_dict = {v['valve_id']: v['openness_percent'] for v in valve_openness_list}
            
            # æ„å»ºä¼ æ„Ÿå™¨æ•°æ®
            sensor_data = {
                'electrode_depths': parsed.get('electrode_depths', {}),
                'cooling': {
                    'flows': parsed.get('cooling_flows', {}),
                    'pressures': parsed.get('cooling_pressures', {}),
                    'pressure_diff': parsed.get('filter_pressure_diff', {}),
                    'cover_total': parsed.get('furnace_cover_total_volume', 0.0),  # ç‚‰ç›–ç´¯è®¡æµé‡
                    'shell_total': parsed.get('furnace_shell_total_volume', 0.0),  # ç‚‰çš®ç´¯è®¡æµé‡
                },
                'hopper': _latest_weight_data.copy() if _latest_weight_data else {},
                'valve_status': parsed.get('valve_status', {}),
                'valve_openness': valve_openness_dict,
                'energy_total': 0.0,  # å ä½ï¼Œåç»­ä»åŠŸç‡è®¡ç®—å™¨è·å–
                'timestamp': time.time()
            }
            
            # è·å–æœ€æ–°èƒ½è€—æ•°æ®
            try:
                from backend.services.power_energy_calculator import get_power_energy_calculator
                power_calc = get_power_energy_calculator()
                realtime_power_data = power_calc.get_realtime_data()
                sensor_data['energy_total'] = realtime_power_data.get('energy_total', 0.0)
            except Exception as e:
                pass  # èƒ½è€—æ•°æ®è·å–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
            # å†™å…¥ç¼“å­˜
            data_cache.set_sensor_data(sensor_data)
            
            # å‘é€ä¿¡å·åˆ°å‰ç«¯
            data_bridge.emit_sensor_data(sensor_data)
            
        except Exception as bridge_err:
            print(f" å†™å…¥ DataCache/DataBridge å¤±è´¥: {bridge_err}")
        
        # ========================================
        # 4. è¶é˜€å¼€åº¦è®¡ç®—æœåŠ¡ (æ–°å¢ - æ»‘åŠ¨çª—å£ + è‡ªåŠ¨æ ¡å‡†)
        # ========================================
        try:
            from backend.services.valve_calculator_service import batch_add_valve_statuses
            valve_status_data = parsed.get('valve_status', {})
            valve_status_byte = valve_status_data.get('raw_byte', 0)
            batch_add_valve_statuses(valve_status_byte, datetime.now(timezone.utc))
        except Exception as valve_err:
            print(f" è¶é˜€å¼€åº¦è®¡ç®—å¤±è´¥: {valve_err}")
        
        # 5. è½¬æ¢ä¸º InfluxDB Points (ä¾›å†å²å­˜å‚¨)
        # é‡è¦: åªæœ‰åœ¨æœ‰æ‰¹æ¬¡å·æ—¶æ‰å†™å…¥æ•°æ®åº“ï¼Œé¿å…äº§ç”Ÿæ— æ‰¹æ¬¡çš„æ‚ä¹±æ•°æ®
        now = datetime.now(timezone.utc)
        
        # è·å–å½“å‰æ‰¹æ¬¡å· (ä»…ç”±å‰ç«¯æä¾›ï¼Œåç«¯ä¸è‡ªåŠ¨ç”Ÿæˆ)
        from backend.services.polling_service import ensure_batch_code
        batch_code = ensure_batch_code()
        
        # åªæœ‰åœ¨æœ‰æ‰¹æ¬¡å·æ—¶æ‰å†™å…¥å†å²æ•°æ®åº“
        if batch_code and _furnace_converter:
            dict_points = _furnace_converter.convert_to_points(parsed, now, batch_code)
            _normal_buffer.extend(dict_points)
            
            # ========================================
            # 6. æ·»åŠ å†·å´æ°´ç´¯è®¡é‡ Point (ç”¨äºå†å²æŸ¥è¯¢)
            # ã€ä¼˜åŒ–ã€‘åªåœ¨ç´¯è®¡å€¼å˜åŒ–æ—¶å†™å…¥
            # ========================================
            shell_total = parsed.get('furnace_shell_total_volume', 0.0)
            cover_total = parsed.get('furnace_cover_total_volume', 0.0)
            
            # æ£€æŸ¥æ˜¯å¦å˜åŒ–ï¼ˆå˜åŒ–è¶…è¿‡ 0.01 mÂ³ æ‰å†™å…¥ï¼‰
            shell_changed = abs(shell_total - _prev_furnace_shell_water_total) > 0.01
            cover_changed = abs(cover_total - _prev_furnace_cover_water_total) > 0.01
            
            if shell_changed or cover_changed:
                water_point = {
                    'measurement': 'sensor_data',
                    'tags': {
                        'device_type': 'electric_furnace',
                        'module_type': 'cooling_water_total',
                        'device_id': 'furnace_1',
                        'batch_code': batch_code
                    },
                    'fields': {
                        'furnace_shell_water_total': shell_total,
                        'furnace_cover_water_total': cover_total,
                    },
                    'time': now
                }
                _normal_buffer.append(water_point)
                
                # æ›´æ–°ä¸Šæ¬¡å†™å…¥çš„å€¼
                _prev_furnace_shell_water_total = shell_total
                _prev_furnace_cover_water_total = cover_total
            
            # ========================================
            # 7. æ·»åŠ ç‚‰çš®-ç‚‰ç›–å‹å·®ç»å¯¹å€¼ Point (ç”¨äºå†å²æŸ¥è¯¢)
            # ========================================
            pressure_diff_abs = abs(furnace_shell_pressure - furnace_cover_pressure)
            pressure_diff_point = {
                'measurement': 'sensor_data',
                'tags': {
                    'device_type': 'electric_furnace',
                    'module_type': 'cooling_system',
                    'device_id': 'furnace_1',
                    'metric': 'pressure_diff',
                    'batch_code': batch_code
                },
                'fields': {
                    'value': pressure_diff_abs,
                },
                'time': now
            }
            _normal_buffer.append(pressure_diff_point)
            
    except Exception as e:
        print(f" å¤„ç† DB32 æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def process_arc_data(raw_data: bytes, batch_code: str):
    """å¤„ç† DB1 å¼§æµå¼§å‹æ•°æ® (ç¼“å­˜ + å†™å…¥æ•°æ®åº“)
    
    è®¾å®šå€¼å’Œæ­»åŒºä»…åœ¨å˜åŒ–æ—¶æ‰å†™å…¥æ•°æ®åº“
    æ–°å¢: åŠŸç‡è®¡ç®—å’Œèƒ½è€—ç´¯è®¡
    ä¼˜åŒ–: èƒ½è€—ç´¯è®¡å€¼åªåœ¨å˜åŒ–æ—¶å†™å…¥æ•°æ®åº“
    
    é‡è¦: æ— è®ºæ˜¯å¦æœ‰æ‰¹æ¬¡å·ï¼Œéƒ½ä¼šæ›´æ–°å®æ—¶ç¼“å­˜ï¼ˆä¾›å‰ç«¯æ˜¾ç¤ºï¼‰
          åªæœ‰æœ‰æ‰¹æ¬¡å·æ—¶æ‰å†™å…¥å†å²æ•°æ®åº“
    
    Args:
        raw_data: DB1 åŸå§‹å­—èŠ‚æ•°æ®
        batch_code: å½“å‰æ‰¹æ¬¡å·ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
    """
    global _latest_arc_data, _latest_arc_timestamp
    global _prev_setpoints, _prev_deadzone, _prev_energy_total
    
    if not _db1_parser:
        return

    try:
        # 1. è§£æåŸå§‹æ•°æ®
        parsed = _db1_parser.parse_all(raw_data)
        
        # 2. ä½¿ç”¨ç®€åŒ–è½¬æ¢å™¨ (ç›´æ¥ä½¿ç”¨åŸå§‹å€¼)
        arc_data_obj: ArcDataSimple = convert_db1_arc_data_simple(parsed)
        
        # ========================================
        # 3. è®¡ç®—ä¸‰ç›¸åŠŸç‡ (æ–°å¢)
        # ========================================
        power_calc = get_power_energy_calculator()
        power_result = power_calc.calculate_power(
            arc_current_U=arc_data_obj.phase_U.current_A,
            arc_voltage_U=arc_data_obj.phase_U.voltage_V,
            arc_current_V=arc_data_obj.phase_V.current_A,
            arc_voltage_V=arc_data_obj.phase_V.voltage_V,
            arc_current_W=arc_data_obj.phase_W.current_A,
            arc_voltage_W=arc_data_obj.phase_W.voltage_V,
        )
        
        # 4. æ„å»ºç¼“å­˜æ•°æ® (UVWä¸‰ç›¸ + ä¸‰ä¸ªè®¾å®šå€¼ + æ‰‹åŠ¨æ­»åŒº + åŠŸç‡)
        setpoints = arc_data_obj.get_setpoints_A()
        arc_cache = {
            'parsed': parsed,
            'converted': arc_data_obj.to_dict(),
            'arc_current': {
                'U': arc_data_obj.phase_U.current_A,
                'V': arc_data_obj.phase_V.current_A,
                'W': arc_data_obj.phase_W.current_A,
            },
            'arc_voltage': {
                'U': arc_data_obj.phase_U.voltage_V,
                'V': arc_data_obj.phase_V.voltage_V,
                'W': arc_data_obj.phase_W.voltage_V,
            },
            'power_total': power_result['power_total'],
            'setpoints': {
                'U': setpoints[0],
                'V': setpoints[1],
                'W': setpoints[2],
            },
            'manual_deadzone_percent': arc_data_obj.manual_deadzone_percent,
            'timestamp': arc_data_obj.timestamp
        }
        
        # 5. æ›´æ–°å†…å­˜ç¼“å­˜
        with _data_lock:
            _latest_arc_data = arc_cache
            _latest_arc_timestamp = datetime.now()
        
        # ========================================
        # 5.1 å†™å…¥ç»Ÿä¸€ç¼“å­˜ DataCache + å‘é€ä¿¡å· DataBridge
        # ========================================
        try:
            from backend.bridge.data_cache import get_data_cache
            from backend.bridge.data_bridge import get_data_bridge
            import time
            
            data_cache = get_data_cache()
            data_bridge = get_data_bridge()
            
            # è·å–æœ€æ–°èƒ½è€—æ•°æ®ï¼ˆä½¿ç”¨å·²å¯¼å…¥çš„ get_power_energy_calculatorï¼‰
            realtime_power_data = power_calc.get_realtime_data()
            
            # æ„å»ºå¼§æµæ•°æ®ï¼ˆåŒ…å«èƒ½è€—å’Œç´§æ€¥åœç”µæ•°æ®ï¼‰
            arc_data = {
                'arc_current': arc_cache['arc_current'],
                'arc_voltage': arc_cache['arc_voltage'],
                'power_total': arc_cache['power_total'],
                'energy_total': realtime_power_data.get('energy_total', 0.0),  # æ·»åŠ èƒ½è€—
                'setpoints': arc_cache['setpoints'],
                'manual_deadzone_percent': arc_cache['manual_deadzone_percent'],
                'emergency_stop': parsed.get('emergency_stop', {}),  # æ·»åŠ ç´§æ€¥åœç”µæ•°æ®
                'timestamp': time.time()
            }
            
            # å†™å…¥ç¼“å­˜
            data_cache.set_arc_data(arc_data)
            
            # å‘é€ä¿¡å·åˆ°å‰ç«¯
            data_bridge.emit_arc_data(arc_data)
            
        except Exception as bridge_err:
            print(f" å†™å…¥ DataCache/DataBridge å¤±è´¥: {bridge_err}")
        
        # 6. ä½¿ç”¨å˜åŒ–æ£€æµ‹è½¬æ¢ä¸º InfluxDB å­—æ®µ
        now = datetime.now(timezone.utc)
        change_result = convert_to_influx_fields_with_change_detection(
            arc_data_obj, _prev_setpoints, _prev_deadzone
        )
        arc_fields = change_result['fields']
        
        # 7. æ·»åŠ æ€»åŠŸç‡å­—æ®µåˆ° arc_fields
        arc_fields['power_total'] = power_result['power_total']
        
        # 8: æ›´æ–°ä¸Šä¸€æ¬¡çš„å€¼
        _prev_setpoints = change_result['current_setpoints']
        _prev_deadzone = change_result['current_deadzone']

        # ========================================
        # 9: å†™å…¥å†å²æ•°æ®åº“ï¼ˆä»…åœ¨æœ‰æ‰¹æ¬¡å·æ—¶ï¼‰
        # ========================================
        if batch_code and arc_fields:
            point_dict = {
                'measurement': 'sensor_data',
                'tags': {
                    'device_type': 'electric_furnace',
                    'module_type': 'arc_data',
                    'device_id': 'electrode',
                    'batch_code': batch_code
                },
                'fields': arc_fields,
                'time': now
            }
            _arc_buffer.append(point_dict)
            
            # æ—¥å¿—ï¼šåªåœ¨è®¾å®šå€¼æˆ–æ­»åŒºå˜åŒ–æ—¶è¾“å‡º
            # setpoint_info = ""
            # if change_result['has_setpoint_change']:
            #     setpoint_info = f", è®¾å®šå€¼å˜åŒ–: U={setpoints[0]}A V={setpoints[1]}A W={setpoints[2]}A"
            # if change_result['has_deadzone_change']:
            #     setpoint_info += f", æ­»åŒºå˜åŒ–: {arc_data_obj.manual_deadzone_percent}%"
            
            # print(f" [DB1] å¼§æµå¼§å‹+åŠŸç‡æ•°æ®å·²ç¼“å­˜: Uç›¸å¼§æµ={arc_data_obj.phase_U.current_A}A, "
            #       f"åŠŸç‡={power_result['power_total']:.2f}kW{setpoint_info}")
        
        # ========================================
        # 10. æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—èƒ½è€— (æ¯15ç§’)
        # ã€ä¼˜åŒ–ã€‘åªåœ¨èƒ½è€—ç´¯è®¡å€¼å˜åŒ–æ—¶å†™å…¥
        # ========================================
        # ã€ä¿®å¤ã€‘åªæœ‰åœ¨è¿è¡ŒçŠ¶æ€ä¸”æœ‰æ‰¹æ¬¡å·æ—¶æ‰è®¡ç®—èƒ½è€—
        from backend.services.batch_service import get_batch_service
        batch_service_check = get_batch_service()
        
        if batch_code and batch_service_check.is_running and power_result['should_calc_energy']:
            energy_result = power_calc.calculate_energy_increment()
            current_energy_total = energy_result['energy_total']
            
            # æ£€æŸ¥èƒ½è€—æ˜¯å¦å˜åŒ–ï¼ˆå˜åŒ–è¶…è¿‡ 0.01 kWh æ‰å†™å…¥ï¼‰
            energy_changed = abs(current_energy_total - _prev_energy_total) > 0.01
            
            if energy_changed:
                # å°†èƒ½è€—æ•°æ®æ·»åŠ åˆ°ç¼“å­˜ï¼ˆæ‰¹é‡å†™å…¥ï¼‰
                energy_point = {
                    'measurement': 'sensor_data',
                    'tags': {
                        'device_type': 'electric_furnace',
                        'module_type': 'energy_consumption',
                        'device_id': 'electrode',
                        'batch_code': batch_code
                    },
                    'fields': {
                        'energy_total': current_energy_total,
                    },
                    'time': now
                }
                _arc_buffer.append(energy_point)
                
                # æ›´æ–°ä¸Šæ¬¡å†™å…¥çš„å€¼
                _prev_energy_total = current_energy_total
            
    except Exception as e:
        print(f" å¤„ç† DB1 å¼§æµå¼§å‹æ•°æ®å¤±è´¥: {e}")
        traceback.print_exc()


def process_status_data(raw_data: bytes):
    """å¤„ç† DB30 çŠ¶æ€æ•°æ® (åªç¼“å­˜ï¼Œä¸å†™å…¥æ•°æ®åº“)"""
    global _latest_status_data, _latest_status_timestamp
    
    if not _status_parser:
        return

    try:
        parsed = _status_parser.parse_all(raw_data)
        
        with _data_lock:
            _latest_status_data = parsed
            _latest_status_timestamp = datetime.now()
        
        # ========================================
        # å‘é€è¿æ¥çŠ¶æ€åˆ°å‰ç«¯ (å¯é€‰)
        # ========================================
        try:
            from backend.bridge.data_bridge import get_data_bridge
            data_bridge = get_data_bridge()
            
            # æ£€æŸ¥ PLC è¿æ¥çŠ¶æ€
            plc_connected = parsed.get('plc_connected', False)
            data_bridge.emit_connection_status(plc_connected)
            
        except Exception as bridge_err:
            pass  # çŠ¶æ€æ•°æ®ä¸æ˜¯å…³é”®æ•°æ®ï¼Œå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
    except Exception as e:
        print(f" å¤„ç† DB30 çŠ¶æ€æ•°æ®å¤±è´¥: {e}")


def process_db41_data(raw_data: bytes):
    """å¤„ç† DB41 æ•°æ®çŠ¶æ€ (åªç¼“å­˜ï¼Œä¸å†™å…¥æ•°æ®åº“)"""
    global _latest_db41_data, _latest_db41_timestamp
    
    if not _db41_parser:
        return

    try:
        parsed = _db41_parser.parse_all(raw_data)
        
        with _data_lock:
            _latest_db41_data = parsed
            _latest_db41_timestamp = datetime.now()
            
    except Exception as e:
        print(f" å¤„ç† DB41 æ•°æ®çŠ¶æ€å¤±è´¥: {e}")


# ============================================================
# 3: æ‰¹é‡å†™å…¥ InfluxDB æ¨¡å—
# ============================================================

def process_weight_data(
    weight_result: Dict[str, Any],
    batch_code: str,
    is_discharging: bool = False,
    is_requesting: bool = False
):
    """å¤„ç†æ–™ä»“é‡é‡æ•°æ® (Modbus RTU + PLC æŠ•æ–™ä¿¡å·)
    
    ä¼˜åŒ–: åªåœ¨æŠ•æ–™ç´¯è®¡å€¼å˜åŒ–æ—¶å†™å…¥æ•°æ®åº“ï¼Œé¿å…é‡å¤å†™å…¥ç›¸åŒçš„ç´¯è®¡å€¼
    
    Args:
        weight_result: read_hopper_weight() è¿”å›çš„ç»“æœ
        batch_code: å½“å‰æ‰¹æ¬¡å·
        is_discharging: %Q3.7 ç§¤æ’æ–™ä¿¡å· (True=æ­£åœ¨æŠ•æ–™)
        is_requesting: %Q4.0 ç§¤è¦æ–™ä¿¡å·
    """
    global _latest_weight_data, _latest_weight_timestamp, _prev_feeding_total

    try:
        # 1. æ›´æ–°å†…å­˜ç¼“å­˜ (å§‹ç»ˆæ›´æ–°ï¼Œå³ä½¿æ²¡æœ‰æ‰¹æ¬¡å·)
        with _data_lock:
            _latest_weight_data = weight_result
            _latest_weight_timestamp = datetime.now()
        
        # åªæœ‰æœ‰æ‰¹æ¬¡å·æ—¶æ‰å†™å…¥æ•°æ®åº“
        if not batch_code:
            return
        
        # 2. å¦‚æœè¯»å–æˆåŠŸï¼Œå¤„ç†æŠ•æ–™ç´¯è®¡
        if weight_result.get('success') and weight_result.get('weight') is not None:
            weight_kg = float(weight_result['weight'])
            now = datetime.now(timezone.utc)
            
            # ========================================
            # 2.1 æŠ•æ–™ç´¯è®¡å™¨ï¼šæ·»åŠ æ•°æ®ç‚¹åˆ°é˜Ÿåˆ—
            # ã€ä¿®å¤ã€‘åªæœ‰åœ¨è¿è¡ŒçŠ¶æ€æ—¶æ‰æ·»åŠ æ•°æ®å’Œè®¡ç®—
            # ========================================
            from backend.services.batch_service import get_batch_service
            batch_service = get_batch_service()
            
            feeding_acc = get_feeding_accumulator()
            
            if batch_service.is_running:
                feeding_result = feeding_acc.add_measurement(
                    weight_kg=weight_kg,
                    is_discharging=is_discharging,
                    is_requesting=is_requesting
                )
                
                # 2.2 æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—æŠ•æ–™ (æ¯30ç§’)
                if feeding_result['should_calc']:
                    calc_result = feeding_acc.calculate_feeding()
                    # print(f"ğŸ“Š æŠ•æ–™è®¡ç®—å®Œæˆ: æœ¬æ¬¡æ–°å¢ {calc_result['total_added']:.1f}kg, ç´¯è®¡ {calc_result['feeding_total']:.1f}kg")
            
            # è·å–å½“å‰ç´¯è®¡å€¼
            current_feeding_total = feeding_acc.get_feeding_total()
            
            # æ›´æ–°ç¼“å­˜ä¸­çš„æŠ•æ–™æ€»é‡
            with _data_lock:
                _latest_weight_data['feeding_total'] = current_feeding_total
                _latest_weight_data['is_discharging'] = is_discharging
            
            # ========================================
            # 2.2.1 æ›´æ–° DataCache ä¸­çš„æ–™ä»“æ•°æ®
            # ========================================
            try:
                from backend.bridge.data_cache import get_data_cache
                data_cache = get_data_cache()
                
                # è·å–æœ€æ–°çš„ä¼ æ„Ÿå™¨æ•°æ®å¹¶æ›´æ–°æ–™ä»“éƒ¨åˆ†
                sensor_data = data_cache.get_sensor_data()
                if sensor_data:
                    sensor_data['hopper'] = {
                        'weight': weight_kg,
                        'feeding_total': current_feeding_total,
                        'is_discharging': is_discharging,
                        'is_requesting': is_requesting
                    }
                    sensor_data['timestamp'] = datetime.now().timestamp()
                    data_cache.set_sensor_data(sensor_data)
            except Exception as cache_err:
                pass  # ç¼“å­˜æ›´æ–°å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
            # ========================================
            # 2.3 è½¬æ¢ä¸º InfluxDB Point
            # ã€ä¼˜åŒ–ã€‘åªåœ¨æŠ•æ–™ç´¯è®¡å€¼å˜åŒ–æ—¶å†™å…¥ feeding_total
            # ========================================
            if batch_service.is_running:
                # æ£€æŸ¥ç´¯è®¡å€¼æ˜¯å¦å˜åŒ–ï¼ˆå˜åŒ–è¶…è¿‡ 0.1kg æ‰å†™å…¥ï¼‰
                feeding_total_changed = abs(current_feeding_total - _prev_feeding_total) > 0.1
                
                # æ„å»º fields
                fields = {'net_weight': weight_kg}
                
                # åªåœ¨ç´¯è®¡å€¼å˜åŒ–æ—¶æ·»åŠ  feeding_total å­—æ®µ
                if feeding_total_changed:
                    fields['feeding_total'] = current_feeding_total
                    _prev_feeding_total = current_feeding_total  # æ›´æ–°ä¸Šæ¬¡å†™å…¥çš„å€¼
                
                point_dict = {
                    'measurement': 'sensor_data',
                    'tags': {
                        'device_type': 'electric_furnace',
                        'module_type': 'hopper_weight',
                        'device_id': 'hopper_1',
                        'batch_code': batch_code
                    },
                    'fields': fields,
                    'time': now
                }
                
                _normal_buffer.append(point_dict)
            
    except Exception as e:
        print(f" å¤„ç†æ–™ä»“é‡é‡æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# ============================================================
# æ‰¹é‡å†™å…¥ InfluxDB
# ============================================================
async def flush_arc_buffer():
    """æ‰¹é‡å†™å…¥ DB1 å¼§æµå¼§å‹ç¼“å­˜
    
    æ³¨æ„: åªæœ‰åœ¨è¿è¡ŒçŠ¶æ€ (is_running=True) æ—¶æ‰å†™å…¥æ•°æ®åº“
    ç»ˆæ­¢å†¶ç‚¼åï¼ˆPAUSEDçŠ¶æ€ï¼‰ä¸å†™å…¥æ•°æ®åº“
    æ–­ç”µæ¢å¤åçŠ¶æ€ä¸º runningï¼Œä¼šç»§ç»­å†™å…¥æ•°æ®
    """
    global _stats, _arc_buffer
    
    if not _arc_buffer:
        return
    
    # æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€ - åªæœ‰è¿è¡Œä¸­ï¼ˆRUNNINGï¼‰æ‰å†™æ•°æ®åº“
    from backend.services.batch_service import get_batch_service
    batch_service = get_batch_service()
    
    if not batch_service.is_running:
        # æœªè¿è¡Œæ—¶ï¼ˆIDLE/PAUSED/STOPPEDï¼‰ï¼Œæ¸…ç©ºç¼“å­˜ä½†ä¸å†™å…¥
        skipped_count = len(_arc_buffer)
        _arc_buffer.clear()
        if skipped_count > 0:
            print(f"â¸ï¸ [DB1] è·³è¿‡å†™å…¥ {skipped_count} ä¸ªæ•°æ®ç‚¹ (çŠ¶æ€: {batch_service.state.value})")
        return
    
    dict_points_list = list(_arc_buffer)
    _arc_buffer.clear()
    
    influx_points = []
    for dp in dict_points_list:
        p = build_point(dp['measurement'], dp['tags'], dp['fields'], dp['time'])
        if p:
            influx_points.append(p)
            
    if not influx_points:
        return

    try:
        success, err = write_points_batch(influx_points)
        if success:
            _stats["successful_writes"] += len(influx_points)
            print(f" [DB1] æ‰¹é‡å†™å…¥æˆåŠŸ: {len(influx_points)} ä¸ªæ•°æ®ç‚¹")
        else:
            _stats["failed_writes"] += len(influx_points)
            print(f" [DB1] æ‰¹é‡å†™å…¥å¤±è´¥: {err}")
        
    except Exception as e:
        _stats["failed_writes"] += len(influx_points)
        print(f" [DB1] æ‰¹é‡å†™å…¥å¼‚å¸¸: {e}")


async def flush_normal_buffer():
    """æ‰¹é‡å†™å…¥ DB32/é‡é‡ç¼“å­˜
    
    æ³¨æ„: åªæœ‰åœ¨è¿è¡ŒçŠ¶æ€ (is_running=True) æ—¶æ‰å†™å…¥æ•°æ®åº“
    ç»ˆæ­¢å†¶ç‚¼åï¼ˆPAUSEDçŠ¶æ€ï¼‰ä¸å†™å…¥æ•°æ®åº“
    æ–­ç”µæ¢å¤åçŠ¶æ€ä¸º runningï¼Œä¼šç»§ç»­å†™å…¥æ•°æ®
    """
    global _stats, _normal_buffer
    
    if not _normal_buffer:
        return
    
    # æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€ - åªæœ‰è¿è¡Œä¸­ï¼ˆRUNNINGï¼‰æ‰å†™æ•°æ®åº“
    from backend.services.batch_service import get_batch_service
    batch_service = get_batch_service()
    
    if not batch_service.is_running:
        # æœªè¿è¡Œæ—¶ï¼ˆIDLE/PAUSED/STOPPEDï¼‰ï¼Œæ¸…ç©ºç¼“å­˜ä½†ä¸å†™å…¥
        skipped_count = len(_normal_buffer)
        _normal_buffer.clear()
        if skipped_count > 0:
            print(f"â¸ï¸ [DB32] è·³è¿‡å†™å…¥ {skipped_count} ä¸ªæ•°æ®ç‚¹ (çŠ¶æ€: {batch_service.state.value})")
        return
    
    dict_points_list = list(_normal_buffer)
    _normal_buffer.clear()
    
    influx_points = []
    for dp in dict_points_list:
        p = build_point(dp['measurement'], dp['tags'], dp['fields'], dp['time'])
        if p:
            influx_points.append(p)
            
    if not influx_points:
        return

    try:
        success, err = write_points_batch(influx_points)
        if success:
            _stats["successful_writes"] += len(influx_points)
            print(f" [DB32] æ‰¹é‡å†™å…¥æˆåŠŸ: {len(influx_points)} ä¸ªæ•°æ®ç‚¹")
        else:
            _stats["failed_writes"] += len(influx_points)
            print(f" [DB32] æ‰¹é‡å†™å…¥å¤±è´¥: {err}")
        
    except Exception as e:
        _stats["failed_writes"] += len(influx_points)
        print(f" [DB32] æ‰¹é‡å†™å…¥å¼‚å¸¸: {e}")


# ============================================================
# 4: ç¼“å­˜æ•°æ®è·å–å‡½æ•°æ¨¡å— (ä¾› API è°ƒç”¨)
# ============================================================
def get_latest_modbus_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ DB32 ä¼ æ„Ÿå™¨æ•°æ®"""
    with _data_lock:
        return {
            'data': _latest_modbus_data.copy() if _latest_modbus_data else {},
            'timestamp': _latest_modbus_timestamp.isoformat() if _latest_modbus_timestamp else None
        }


def get_latest_arc_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ DB1 å¼§æµå¼§å‹æ•°æ®"""
    with _data_lock:
        return {
            'data': _latest_arc_data.copy() if _latest_arc_data else {},
            'timestamp': _latest_arc_timestamp.isoformat() if _latest_arc_timestamp else None
        }


def get_latest_status_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ DB30 é€šä¿¡çŠ¶æ€æ•°æ®"""
    with _data_lock:
        return {
            'data': _latest_status_data.copy() if _latest_status_data else {},
            'timestamp': _latest_status_timestamp.isoformat() if _latest_status_timestamp else None
        }


def get_latest_db41_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ DB41 æ•°æ®çŠ¶æ€"""
    with _data_lock:
        return {
            'data': _latest_db41_data.copy() if _latest_db41_data else {},
            'timestamp': _latest_db41_timestamp.isoformat() if _latest_db41_timestamp else None
        }


def get_latest_weight_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„æ–™ä»“é‡é‡æ•°æ®"""
    with _data_lock:
        return {
            'data': _latest_weight_data.copy() if _latest_weight_data else {},
            'timestamp': _latest_weight_timestamp.isoformat() if _latest_weight_timestamp else None
        }


def get_latest_electricity_data() -> Dict[str, Any]:
    """è·å–æœ€æ–°çš„ç”µè¡¨æ•°æ®
    
    æ³¨æ„: å½“å‰ç‰ˆæœ¬æ— ç‹¬ç«‹ç”µè¡¨é‡‡é›†ï¼Œè¿”å›ç©ºæ•°æ®ã€‚
    ç”µåŠ›ç›¸å…³æ•°æ®è¯·ä½¿ç”¨ get_latest_arc_data() è·å–å¼§æµå¼§å‹ã€‚
    """
    return {
        'data': {
            'converted': {
                'Pt': 0.0,       # æ€»åŠŸç‡ kW (æš‚æ— )
                'Ua_0': 0.0,     # Aç›¸ç”µå‹ V (æš‚æ— )
                'I_0': 0.0,      # Aç›¸ç”µæµ A (æš‚æ— )
                'I_1': 0.0,      # Bç›¸ç”µæµ A (æš‚æ— )
                'I_2': 0.0,      # Cç›¸ç”µæµ A (æš‚æ— )
                'ImpEp': 0.0,    # ç´¯è®¡ç”µèƒ½ kWh (æš‚æ— )
            },
            'summary': {},
            'ct_ratio': 20,
        },
        'timestamp': None
    }


def get_valve_status_queues() -> Dict[int, List[Dict[str, Any]]]:
    """è·å–4ä¸ªè¶é˜€çš„çŠ¶æ€é˜Ÿåˆ—"""
    with _data_lock:
        result = {}
        for valve_id in range(1, 5):
            status_list = list(_valve_status_queues[valve_id])
            timestamp_list = list(_valve_status_timestamps[valve_id])
            
            result[valve_id] = [
                {
                    "status": status,
                    "timestamp": ts,
                    "state_name": _parse_valve_state_name(status)
                }
                for status, ts in zip(status_list, timestamp_list)
            ]
        
        return result


def _parse_valve_state_name(status: str) -> str:
    """è§£æè¶é˜€çŠ¶æ€åç§°"""
    state_map = {
        "10": "closed",
        "01": "open",
        "11": "error",
        "00": "unknown"
    }
    return state_map.get(status, "unknown")


def get_buffer_status() -> Dict[str, Any]:
    """è·å–ç¼“å­˜çŠ¶æ€"""
    return {
        'arc_buffer_size': len(_arc_buffer),
        'normal_buffer_size': len(_normal_buffer),
        'arc_batch_size': _arc_batch_size,
        'normal_batch_size': _normal_batch_size,
        'stats': _stats.copy()
    }


def update_stats(key: str, value: Any):
    """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
    global _stats
    _stats[key] = value


# ============================================================
# DB18 æ–™ä»“ç”µæ°”æ•°æ®å¤„ç†æ¨¡å—
# ============================================================
def process_hopper_db18_data(raw_data: bytes):
    """å¤„ç† DB18 æ–™ä»“ç”µæ°”æ•°æ®
    
    ä¸»è¦åŠŸèƒ½: è§£ææ–™ä»“ä¸Šé™å€¼
    
    Args:
        raw_data: DB18 åŸå§‹å­—èŠ‚æ•°æ®
    """
    global _latest_hopper_upper_limit, _latest_hopper_upper_limit_timestamp
    
    if not _db18_parser:
        return
    
    try:
        # è§£æ DB18 æ•°æ®
        parsed = _db18_parser.parse(raw_data)
        
        # æå–æ–™ä»“ä¸Šé™å€¼
        upper_limit = parsed.get('upper_limit', 4900.0)
        
        # æ›´æ–°å†…å­˜ç¼“å­˜
        with _data_lock:
            _latest_hopper_upper_limit = upper_limit
            _latest_hopper_upper_limit_timestamp = datetime.now()
        
    except Exception as e:
        print(f"å¤„ç† DB18 æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def get_hopper_upper_limit() -> float:
    """è·å–æ–™ä»“ä¸Šé™å€¼
    
    Returns:
        æ–™ä»“ä¸Šé™å€¼ (kg)
    """
    with _data_lock:
        return _latest_hopper_upper_limit
